---
layout: ../../layouts/MarkdownPostLayout.astro
title: 'Links and notes on GPT-3'
pubDate: 2023-10-15
description: 'papers, links, architecture etc'
author: 'Jeff'
image:
    url: ""
    alt: ""
tags: []
---
***
* Original [GPT-3 paper](https://arxiv.org/abs/2005.14165) "Language Models are Few-Shot Learners" June 2020 by Brown, Mann, Ryder,...Jack Clark, Ilya Sutskever
* [2022 Survey](https://arxiv.org/abs/2212.00857) by Mingyu Zong and Bhaskar Krishnamachari
* Lambda Lab's [Overview of GPT-3](https://lambdalabs.com/blog/demystifying-gpt-3)
* PDF of [GPT-3: Its Nature, Scope, Limits, and Consequences](https://link.springer.com/article/10.1007/s11023-020-09548-1) by Luciano Floridi & Massimo Chiriatti, November 2020.



***

## Twilio
* Response to "Assume I am a machine learning expert who has good familiarty with deep learning. Please write a technical description of GPT-3 architecture with citations to most signicant historical academic papers that were antecedents to the primary GPT-3 2020 paper 'Language Models are Few-Shot Learners'."
	* [Cached link to phind.com](https://www.phind.com/search?cache=dwjzm6a4ofeu49gi5epamfoo)
	* **Be sure to read cached link. It's quite good!**
* Twilio's [guide to GPT-3](https://www.twilio.com/blog/ultimate-guide-openai-gpt-3-language-model)
* Sample generated by GPT-3 within the context of the article.
	* 'GPT-3 is a deep neural network that uses the attention mechanism to predict the next word in a sentence. It is trained on a corpus of over 1 billion words, and can generate text at character level accuracy. 
	* 'GPT-3's architecture consists of two main components: an encoder and a decoder. The encoder takes as input the previous word in the sentence and produces a vector representation of it, which is then passed through an attention mechanism to produce the next word prediction. 
	* 'The decoder takes as input both the previous word and its vector representation, and outputs a probability distribution over all possible words given those inputs.
	* 'GPT-3's performance is on par with the best language models for text generation, which is significantly better than previous GPT models. Microsoft's Turing NLG model can generate text at character level accuracy on a test set of Wikipedia articles, but requires an enormous amount of training data to do so. 
	* 'OpenAI claims that GPT-3 can achieve this level of performance without any additional training data after its initial pre-training period. In addition, GPT-3 is capable of generating longer sentences and paragraphs than earlier models such as Google's BERT and Stanford NLP's Transformer.'



***
## CUP robert dale article, Dec 2020
* Quote below is from Robert Dale's [GPT-3: What's it good for?](https://www.cambridge.org/core/journals/natural-language-engineering/article/gpt3-whats-it-good-for/0E05CFE68A7AC8BF794C8ECBE28AA990). Published by Cambridge University Press, December 2020:

### 2. Some History
OpenAI was founded as a non-profit research organisation in late 2015 via a collective pledge of US$1B from a group of industry heavyweights...

Over its first few years, the organisation publicly released a number of software artefacts, but nothing that made headlines outside of the relevant communities of interest.

Then, in **February 2019, OpenAI announced GPT-2 (for Generative Pre-trained Transformer 2)**, a large unsupervised transformer language model with 1.5B parameters trained on 40GB of text, or roughly 10B tokens. When used to repeatedly predict the next word in a text based on the preceding context, the model was capable of generating very coherent and plausible-sounding output, although it was also capable of outputting gibberish.

In its announcement, OpenAI stated ‘Due to our concerns about malicious applications of the technology, we are not releasing the trained model’. This immediately drew criticism...

In any case, the full 1.5B parameter model was eventually released in November 2019, following intermediate releases embodying increasingly larger language models: a ‘small’ 124M parameter model in February, a medium 355M model in May, and a 774M model in August.


#### 3. GPT-3: A generator to trump all others
In June 2020, OpenAI announced GPT-3, a new language model more than 100 times larger than GPT-2, with 175B parameters and 96 layers trained on a corpus of 499B tokens of web content, **making it by far the largest language model constructed to date**. At the time of writing, the closest contenders are considerably smaller, with Microsoft’s T-NLG and Google’s T5-11B both being less than a tenth of GPT-3’s size. And size, it seems, does matter: as it turned out, the texts created by GPT-3 were much more likely to sound coherent than those of its predecessor.

Again, the model itself was not made available; instead, access was to be provided via an API, thus giving the model’s creators more control over its use. At the time of writing, a beta version of the API is up and running, but you’ll have to get on the presumably rather long wait-list if you want access. In the interim, some information on future pricing has leaked: via a typical SaaS tiered pricing model, there’s a free tier that gets you 100,000 generated tokens, a US$100-per-month tier that gets you 2M tokens, and a US$400-per-month tier that gets you 10M.

These prices have been criticised as being on the high side; they’re certainly more than the sub-US$50 per month price-point that’s typical of many other SaaS offerings and steep enough to lock out all but the keenest lone researchers. On the other hand, there are no obvious comparators on the basis of which we might establish what counts as a reasonable price, and arguably it’s actually pretty cheap for access to a language model that is estimated to have a compute cost of US$4.6M per training run – and that’s only a fraction of the overall total development and running costs.

Meanwhile, back in March 2019, the non-profit OpenAI had restructured as a ‘capped-profit’ company, the stated reason being that this was necessary to be able to raise the kind of capital required to fund the company’s high cost of research and maintain a pace of development competitive with major industry players like Google. Following this change, in July 2019, Microsoft agreed to invest US$1B in OpenAI over the next decade; and just over a year later, in September 2020, Microsoft obtained an exclusive licence to GPT-3. The consequences of this deal are unclear, but it’s likely that the API access will be unaffected, whereas Microsoft’s customers might eventually see the benefits of GPT-3 in a range of applications effectively for free.


There’s been a lot of praise for the technology’s capabilities, especially in text generation. The typical use of the API involves providing a prompt and some initial text to get the model going, along with some optional parameter fiddling. Some of the outputs produced are truly breathtaking in their plausibility and believability as candidates for being human-authored text...

Apart from the obvious application of text generation, the technology has also been lauded for its results in a wide range of other areas, some quite surprising: so you’ll easily find examples and discussion of the model’s capability in generating poetry, playing chess, doing arithmetic and writing web interface code on the basis of requirements expressed in natural language. 

