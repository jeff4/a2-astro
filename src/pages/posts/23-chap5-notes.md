---
layout: ../../layouts/MarkdownPostLayout.astro
title: 'Goodfellow (GBC) Chapter 5'
pubDate: 2023-11-11
description: 'Quick review of ML basics esp. statistics and related math'
author: 'Jeff'
image:
    url: ""
    alt: ""
tags: []
---


**Deep Learning (2016)** 
* Ian Goodfellow, Yoshua Bengio, and Aaron Courville
* GBC
## Chapter 5
### 5.1 Learning Algorithms
* Section 5.1.1 - The Task *T* can be many things: classification, classification with missing errors, regression, transcription, machine translation, structured output, anomaly detection, synthesis/sampling, imputation of missing values, denoising, density estimation *aka* probability mass function estimation.
* Section 5.1.2 - The Performance Measure *P*. Accuracy + Error Rate = 1.0
	* Reserving a **test set** p. 101
* Section 5.1.3 - The Experience *E*
	* Can be broadly categorized into supervised, unsupervised, and reinforcement learning
	* Rather fuzzy boundary between supervised and unsupervised learning at times. 
	* p. 103. Concept of **design matrix** which is just an object to hold many examples at once. Eg., if there are 150 plants each of which has 4 features, then you can hold that in a matrix of 4 columns and 150 rows of plants (the famous Iris leaf dataset)
* 5.1.4 - Example of Linear Regression and trying to reduce the MSE (Mean Squared Error)
	* Not just about y = **wx**, but also with the bias parameter: y = **wx** + b for the 'y-intercept'.
### 5.2 Capacity, Overfitting, and Underfitting
* p. 107 'Again we want to reduce the *training error*; so far what we have described is just an optimization problem like in linear regression.'
* '**Critical point** What separates machine learning from vanilla optimization is that we want the **test error* *aka* the **generalization error** to be reduced as well.
	* 'The definition of generalization error *aka* test error is **the expected value of the error on a *new* input.**
	* 'Here the expectation is taken across different possible inputs drawn from the distribution of inputs we expect the system to encounter in practice.
* p. 108 here is where we show why it's important there is a general similarity between the examples in the training dataset and the test data set. We must use the **i.i.d.** assumptions; the examples are *independent* of each other in the same way scross both training and test data sets; *and* the examples are *identically distributed* aka drawn from the exact same probability distribution
* Another way of expressing this is that the examples in both the training and test data sets are created by a *data-generating process*. And if the probability distribution of output examples is identical across both training and test sets, then for our purposes, we can statistically treat these two sets as being generated by the same data-gen process.
	* We call the underlying distribution the **data-generating distribution** denoted *p*<sub>data</sub>.
* The probabilisitic framework and the i.i.d. assumptions enable us to mathematically study the relationship between training error and test error (aka between training error and generalization error).
* If we are *not* engaging in ML, then given the assumptions above, we training error = test error. However, in ML, *when we use an ML algorithm, we do **not** fix the parameters ahead of time.* First, we sample the training set, then we use that to choose the parameters to reduce the training set error iteratively. Only then do we sample from the test data set. 
	* Under this process, the expected test error is greater than or equal to the expected value of traiing error.
* The factors determining how well a machine learning algorithm will perform are its ability to:
	1. Make the training error *small*
	1. Make the gap between training error and test error (generalization error) *small*
* More on factors
	* Factor one: make the training error too large = underfitting
	* Factor two: make the gap between training and test error too large = overfitting
#### Model Capacity
* p. 109 We can control whether a model is more likely to overfit or underfit by altering the **capacity**. Loosely defined, a model's capacity is its ability to fit a wide variety of functions.
	* Models with low capacity may struggle to fit the training set.
	* Models with high capacity can overfit by memorizing properties of the training set that do not serve them well in the test set, thereby having higher than desired generatlization error.
* One way ot control the capacity of a learning algorithm is by choosing its **hypothesis space**. 
	* i.e., the set of functions that the learning algorithm is allowed to select as being the solution.
	* e.g., the linear regression algorithm has as its hypothesis space the set of all linear functions. To increase this model's capacity, we can expand the hypothesis space to include all polynomial functions as well. 
* Choice of model / hypothesis space is not the only way to expand capacity. Note the difference between **effective capacity** and **representational capacity** as shown between p. 110 and 111.
* p.111 VC dimension aka **Vapnik-Chervonenkis dimension** measures the capacity of a binary classifer.
* p.112 chart on difference between Training Error and Generalization Error. 

### 5.2.1 The No Free Lunch Theorem
* p.113 Wolpert 1996 proved the 'no free lunch theorem for machine learning' which states that, averaged over all possible data-generating distributions, every classification algorithm has the same error rate when classifyig previously unobserved points.
	* i.e., no ML algorithm is universally better than any other.
	* The most sophisticated algorithm we can concieve of us has the same avg performance (over all possible tasks) as merely predicting that every point belongs to the same class.
* p.115 Fortunately, these results hold only when we average over *all* possible data-generating distributions. If we make some assumptions about the kinds of probability distributions we encounter in the real world, then we can design learning algorithms that peform well on those particular distributions.
*  This means that the goal of machine learning research is not to seek a universal learning algorithm or the absolute best learning algorithm. Instead, our goal is to understand what kinds of distributions are relevant to the “real world” that an AI agent experiences, and what kinds of machine learning algorithms perform well on data drawn from the kinds of data-generating distributions we care about.

### 5.2.1 Regularization
* p.115 The only method for modifying a learning algorithm discussed so far is by increasing or decreasing a model's representational capacity by increasing or decreasing the size of the hypothesis space from which we choose the "best" function. i.e., by adding or removing more and fewer functions from the set of possible solutions.
* However, the representational capacity of a model can be changed not only by the raw size of the hypothesis space, but also by the weighting or preference we give to certain types of functions contained in that space.
* For example, we can put our thumb on the scale such that our ML algorithm 'prefers' polynomial functions versus linear functions. In that case, if a polynomial and a linear function are both eligible to solve our problem, the linear function will be chosen only if the linear function fits the training data significantly more than the closest polynomial function does.
* Example of using **weight decay**. Assume we want to minimize the objective function J(w&#8407;) where J() = Mean Squared Error for training + preference for weights to have a smaller L<sup>2</sup> norm. See Equation 5.18 on p.116. 
	* When &#955; = 0, we impose no preference. 
	* As &#955; gets larger, it forces weights to become smaller.
* More generally, we can regularize a model that learns a function f(**x**&#8407;;**&#952;**) by adding a penalty called a **regularizer** to the cost function. In the case of weight decay above, the regularizer is omega(w&#8407;) = w&#8407;<sup>T</sup>w&#8407;. Essentially everything except for the &#955; lambda scalar multiplier.
* Expressing preferences for one function over another is a 'gentler' more general way of controlling a model's capacity than 'violently' including or excluding functions from a hypothesis space. We can think of excluding a function from a hypothesis space as expressing an infinitely strong preference against that function.
* p.117 The philosophy of deep learning is that for a wide range of tasks–and indeed all intellectual tasks that people can do–may all be solved effectively using very general-purpose forms of regularization.

### 5.3 Hyperparameters and Validation Size
* Most ML algos have hyperparamters aka settings that we use to control the algorithm's behavior. The values of hyperparameters are *not* adjusted via the learning algorithm itself.
* Of course, we can have a nested learning procedure whereby one ML algorithm learns the best hyperparameters for another ML algorithm.
* The polynomial regression example in Figure 5.2 on p.110 has as single hyperparamter: the degree of the polynomial which acts as a capacity hyperparamter.
* p.118 Sometimes a setting is considered a hyperparameter because it is just too difficult to optimize; i.e., the learning algorithm doesn't have a path to improve it over time. 
* More frequently, a setting is considered a hyperparameter b/c it's not appropriate to learn that hyperparameter in the training set. 
	* This applies to all hyperparameters that control model capacity.
	* If a setting that managed model capacity was learned on the training set, **that setting would always choose the maximum possible model capacity** aka always resulting in overfitting.
	* For example, we can always ﬁt the training set better with a higher-degree polynomial and a weight decay setting of λ= 0 than we could witha lower-degree polynomial and a positive weight decay setting.

#### Validation Set
* This helps solve the problem of overfitting.
* Before, we discussed how a held-out test set, composed of examples coming from the same distribution as the training set, can be used to estimate the generalization error of a learner, after the learning process has completed. 
* It is important that the test examples are not used in any way to make choices about the model, including its hyperparameters. 
* **For this reason, no example from the test set can be used in the validation set.** Therefore, we always construct the validation set from the *training* data. 
	* i.e., **these 3 datasets are unique and disjoint: (1) training set, (2) validation set, and (3) test set.**
* Speciﬁcally, we split the training data into two disjoint subsets. One of these subsets is used to learn the parameters. 
* The other subset is our validation set, used to estimate the generalization error during or after training,allowing for the hyperparameters to be updated accordingly. The subset of dataused to learn the parameters is still typically called the training set, even though this may be confused with the larger pool of data used for the entire training process. 
* The subset of data used to guide the selection of hyperparameters is called the validation set. 
* Typically, one uses about 80 percent of the training data for training and 20 percent for validation. 
* Since the validation set is used to “train” the hyperparameters, the validation set error will underestimate thegeneralization error, though typically by a smaller amount than the training errordoes. 
* After all hyperparameter optimization is complete, the generalization errormay be estimated using the test set.
* JH: Note that validation set is a subset of the original training set. e.g., if there are 100 examples in the original training set pre-segregation, we might reserve 80 examples for the post-segregration new training set for actual training. And the remaining 20 examples will be used for the validation set.
* p.118 In practice, when the same test set has been used repeatedly to evaluate performance of different algorithms over many years, and especially if we consider all the attempts from the scientiﬁc community at beating the reported state-of-the-art performance on that test set, we end up having optimistic evaluations with the test set as well. 
* Benchmarks can thus become stale and then do not reﬂect the true ﬁeld performance of a trained system. Thankfully, the community tends to move on to new (and usually more ambitious and larger) benchmark datasets.

#### 5.3.1 Cross-validation
* If your original dataset has 100,000s of examples, don't really worry about how you split it into training, validation, and test datasets.
* However, if your original pool of data is too small, it's hard to prove good generalizability. This is when alternative procedures let one use all the examples in the estimation of the mean test error at the price of increased computational cost.
	* These procedures are based on the idea of repeating the training and test computation on different randomely chosen subsets or splits in the original dataset.
	* The most common of these is the *k*-fold cross-valdiation procedure shown on p.120 (Algorithm 5.1) formed by *k* number of non-overlapping subsets.
	* This solution is somewhat imperfect see Bengio and Grandvalet (2004), but approximations are still used.


### 5.4 Estimators, Bias, and Variance
* p.118 The field of statistics provides many tools to not only improve a task during training but to let that algorithm generalize. Let's consider parameter estimation, bias, and variance.

#### 5.4.1 Point Estimation
* *Point estimation* is the attempt to provide the single 'best' prediction of some quantity of interest. In general, the quantity of interest can be a single parameter of a vector of parameters in some parametric model such as the weights in our linear regression example in section 5.1.4., but it can also be a whole function.
* A parameter **&#952;** can have a 'point estimate version' denoted with a hat on top aka theta-hat. Just as a unit vector v with a hat on top is called a 'v-hat'.
* Let { x&#8407;<sub>1</sub>,  x&#8407;<sub>2</sub>,  x&#8407;<sub>3</sub>, ...  x&#8407;<sub>m</sub>} be a set of *m* i.i.d. data points. A *point estimator* is any function `g()` that can take in that list of vectors as input and returns a value **&#952;-hat** 
* While almost any function qualifies as a point estimator based on the above definition, we are looking for *good* estimators. In that case, we want the output **&#952;-hat** to be close to the true underlying **&#952;**.So only estimator functions that output a reasonable **&#952;-hat** will be considered for our desired point estimator function.

#### Function Estimators
* Distinguised from point estimators above because we are not trying to just estimate a single point, but an entire function.



### DELETE BELOW 
* **&#952;** = theta 
* **&#952;-hat** = theta-hat 
* "tp = **&#952;-hat**
* "p = &#8407; = vector arrow above prior character 
* function f(**x**&#8407;;**&#952;**) regularizer** to the cost function. In the case of weight decay above, the regularizer is omega(w&#8407;) = w&#8407;<sup>T</sup>w&#8407;. Essentially everything except for the &#955; lambda scalar multiplier.





### 5.5 Maximum Likelihood Estimation
### 5.6 Bayesian Statistics
### 5.7 Supervised Learning Algorithm
### 5.8 Unsupervised Learning Algorithm
### 5.9 Stochastic Gradient Descent
### 5.10 Building an ML Algoirthm
### 5.11 Challenges that motivated deep learning




















