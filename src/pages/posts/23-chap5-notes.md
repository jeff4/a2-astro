---
layout: ../../layouts/MarkdownPostLayout.astro
title: 'Goodfellow (GBC) Chapter 5'
pubDate: 2023-11-11
description: 'Quick review of ML basics esp. statistics and related math'
author: 'Jeff'
image:
    url: ""
    alt: ""
tags: []
---


**Deep Learning (2016)** 
* Ian Goodfellow, Yoshua Bengio, and Aaron Courville
* GBC
## Chapter 5
### 5.1 Learning Algorithms
* Section 5.1.1 - The Task *T* can be many things: classification, classification with missing errors, regression, transcription, machine translation, structured output, anomaly detection, synthesis/sampling, imputation of missing values, denoising, density estimation *aka* probability mass function estimation.
* Section 5.1.2 - The Performance Measure *P*. Accuracy + Error Rate = 1.0
	* Reserving a **test set** p. 101
* Section 5.1.3 - The Experience *E*
	* Can be broadly categorized into supervised, unsupervised, and reinforcement learning
	* Rather fuzzy boundary between supervised and unsupervised learning at times. 
	* p. 103. Concept of **design matrix** which is just an object to hold many examples at once. Eg., if there are 150 plants each of which has 4 features, then you can hold that in a matrix of 4 columns and 150 rows of plants (the famous Iris leaf dataset)
* 5.1.4 - Example of Linear Regression and trying to reduce the MSE (Mean Squared Error)
	* Not just about y = **wx**, but also with the bias parameter: y = **wx** + b for the 'y-intercept'.
### 5.2 Capacity, Overfitting, and Underfitting
* p. 107 'Again we want to reduce the *training error*; so far what we have described is just an optimization problem like in linear regression.'
* '**Critical point** What separates machine learning from vanilla optimization is that we want the **test error* *aka* the **generalization error** to be reduced as well.
	* 'The definition of generalization error *aka* test error is **the expected value of the error on a *new* input.**
	* 'Here the expectation is taken across different possible inputs drawn from the distribution of inputs we expect the system to encounter in practice.
* p. 108 here is where we show why it's important there is a general similarity between the examples in the training dataset and the test data set. We must use the **i.i.d.** assumptions; the examples are *independent* of each other in the same way scross both training and test data sets; *and* the examples are *identically distributed* aka drawn from the exact same probability distribution
* Another way of expressing this is that the examples in both the training and test data sets are created by a *data-generating process*. And if the probability distribution of output examples is identical across both training and test sets, then for our purposes, we can statistically treat these two sets as being generated by the same data-gen process.
	* We call the underlying distribution the **data-generating distribution** denoted *p*<sub>data</sub>.
* The probabilisitic framework and the i.i.d. assumptions enable us to mathematically study the relationship between training error and test error (aka between training error and generalization error).
* If we are *not* engaging in ML, then given the assumptions above, we training error = test error. However, in ML, *when we use an ML algorithm, we do **not** fix the parameters ahead of time.* First, we sample the training set, then we use that to choose the parameters to reduce the training set error iteratively. Only then do we sample from the test data set. 
	* Under this process, the expected test error is greater than or equal to the expected value of traiing error.
* The factors determining how well a machine learning algorithm will perform are its ability to:
	1. Make the training error *small*
	1. Make the gap between training error and test error (generalization error) *small*
* More on factors
	* Factor one: make the training error too large = underfitting
	* Factor two: make the gap between training and test error too large = overfitting
#### Model Capacity
* p. 109 We can control whether a model is more likely to overfit or underfit by altering the **capacity**. Loosely defined, a model's capacity is its ability to fit a wide variety of functions.
	* Models with low capacity may struggle to fit the training set.
	* Models with high capacity can overfit by memorizing properties of the training set that do not serve them well in the test set, thereby having higher than desired generatlization error.
* One way ot control the capacity of a learning algorithm is by choosing its **hypothesis space**. 
	* i.e., the set of functions that the learning algorithm is allowed to select as being the solution.
	* e.g., the linear regression algorithm has as its hypothesis space the set of all linear functions. To increase this model's capacity, we can expand the hypothesis space to include all polynomial functions as well. 
* Choice of model / hypothesis space is not the only way to expand capacity. Note the difference between **effective capacity** and **representational capacity** as shown between p. 110 and 111.
* p.111 VC dimension aka **Vapnik-Chervonenkis dimension** measures the capacity of a binary classifer.
* p.112 chart on difference between Training Error and Generalization Error. 

### 5.2.1 The No Free Lunch Theorem
* p.113 Wolpert 1996 proved the 'no free lunch theorem for machine learning' which states that, averaged over all possible data-generating distributions, every classification algorithm has the same error rate when classifyig previously unobserved points.
	* i.e., no ML algorithm is universally better than any other.
	* The most sophisticated algorithm we can concieve of us has the same avg performance (over all possible tasks) as merely predicting that every point belongs to the same class.
* p.115 Fortunately, these results hold only when we average over *all* possible data-generating distributions. If we make some assumptions about the kinds of probability distributions we encounter in the real world, then we can design learning algorithms that peform well on those particular distributions.
*  This means that the goal of machine learning research is not to seek a universal learning algorithm or the absolute best learning algorithm. Instead, our goal is to understand what kinds of distributions are relevant to the “real world” that an AI agent experiences, and what kinds of machine learning algorithms perform well on data drawn from the kinds of data-generating distributions we care about.

### 5.2.1 Regularization
* p.115 The only method for modifying a learning algorithm discussed so far is by increasing or decreasing a model's representational capacity by increasing or decreasing the size of the hypothesis space from which we choose the "best" function. i.e., by adding or removing more and fewer functions from the set of possible solutions.
* However, the representational capacity of a model can be changed not only by the raw size of the hypothesis space, but also by the weighting or preference we give to certain types of functions contained in that space.
* For example, we can put our thumb on the scale such that our ML algorithm 'prefers' polynomial functions versus linear functions. In that case, if a polynomial and a linear function are both eligible to solve our problem, the linear function will be chosen only if the linear function fits the training data significantly more than the closest polynomial function does.
* Example of using **weight decay**. Assume we want to minimize the objective function J(w&#8407;) where J() = Mean Squared Error for training + preference for weights to have a smaller L<sup>2</sup> norm. See Equation 5.18 on p.116. 
	* When &#955; = 0, we impose no preference. 
	* As &#955; gets larger, it forces weights to become smaller.
* More generally, we can regularize a model that learns a function f(**x**&#8407;;**&#952;**) by adding a penalty called a **regularizer** to the cost function. In the case of weight decay above, the regularizer is omega(w&#8407;) = w&#8407;<sup>T</sup>w&#8407;. Essentially everything except for the &#955; lambda scalar multiplier.
* Expressing preferences for one function over another is a 'gentler' more general way of controlling a model's capacity than 'violently' including or excluding functions from a hypothesis space. We can think of excluding a function from a hypothesis space as expressing an infinitely strong preference against that function.
* p.117 The philosophy of deep learning is that for a wide range of tasks–and indeed all intellectual tasks that people can do–may all be solved effectively using very general-purpose forms of regularization.

### 5.3 Hyperparameters and Validation Size
* Most ML algos have hyperparamters aka settings that we use to control the algorithm's behavior. The values of hyperparameters are *not* adjusted via the learning algorithm itself.
* Of course, we can have a nested learning procedure whereby one ML algorithm learns the best hyperparameters for another ML algorithm.
* The polynomial regression example in Figure 5.2 on p.110 has as single hyperparamter: the degree of the polynomial which acts as a capacity hyperparamter.
* p.118 Sometimes a setting is considered a hyperparameter because it is just too difficult to optimize; i.e., the learning algorithm doesn't have a path to improve it over time. 
* More frequently, a setting is considered a hyperparameter b/c it's not appropriate to learn that hyperparameter in the training set. 
	* This applies to all hyperparameters that control model capacity.
	* If a setting that managed model capacity was learned on the training set, **that setting would always choose the maximum possible model capacity** aka always resulting in overfitting.
	* For example, we can always ﬁt the training set better with a higher-degree polynomial and a weight decay setting of λ= 0 than we could witha lower-degree polynomial and a positive weight decay setting.

#### Validation Set
* This helps solve the problem of overfitting.
* Before, we discussed how a held-out test set, composed of examples coming from the same distribution as the training set, can be used to estimate the generalization error of a learner, after the learning process has completed. 
* It is important that the test examples are not used in any way to make choices about the model, including its hyperparameters. 
* **For this reason, no example from the test set can be used in the validation set.** Therefore, we always construct the validation set from the *training* data. 
	* i.e., **these 3 datasets are unique and disjoint: (1) training set, (2) validation set, and (3) test set.**
* Speciﬁcally, we split the training data into two disjoint subsets. One of these subsets is used to learn the parameters. 
* The other subset is our validation set, used to estimate the generalization error during or after training,allowing for the hyperparameters to be updated accordingly. The subset of dataused to learn the parameters is still typically called the training set, even though this may be confused with the larger pool of data used for the entire training process. 
* The subset of data used to guide the selection of hyperparameters is called the validation set. 
* Typically, one uses about 80 percent of the training data for training and 20 percent for validation. 
* Since the validation set is used to “train” the hyperparameters, the validation set error will underestimate thegeneralization error, though typically by a smaller amount than the training errordoes. 
* After all hyperparameter optimization is complete, the generalization errormay be estimated using the test set.
* JH: Note that validation set is a subset of the original training set. e.g., if there are 100 examples in the original training set pre-segregation, we might reserve 80 examples for the post-segregration new training set for actual training. And the remaining 20 examples will be used for the validation set.
* p.118 In practice, when the same test set has been used repeatedly to evaluate performance of different algorithms over many years, and especially if we consider all the attempts from the scientiﬁc community at beating the reported state-of-the-art performance on that test set, we end up having optimistic evaluations with the test set as well. 
* Benchmarks can thus become stale and then do not reﬂect the true ﬁeld performance of a trained system. Thankfully, the community tends to move on to new (and usually more ambitious and larger) benchmark datasets.

#### 5.3.1 Cross-validation
* If your original dataset has 100,000s of examples, don't really worry about how you split it into training, validation, and test datasets.
* However, if your original pool of data is too small, it's hard to prove good generalizability. This is when alternative procedures let one use all the examples in the estimation of the mean test error at the price of increased computational cost.
	* These procedures are based on the idea of repeating the training and test computation on different randomely chosen subsets or splits in the original dataset.
	* The most common of these is the *k*-fold cross-valdiation procedure shown on p.120 (Algorithm 5.1) formed by *k* number of non-overlapping subsets.
	* This solution is somewhat imperfect see Bengio and Grandvalet (2004), but approximations are still used.


### 5.4 Estimators, Bias, and Variance
* p.118 The field of statistics provides many tools to not only improve a task during training but to let that algorithm generalize. Let's consider parameter estimation, bias, and variance.

#### 5.4.1 Point Estimation
* *Point estimation* is the attempt to provide the single 'best' prediction of some quantity of interest. In general, the quantity of interest can be a single parameter of a vector of parameters in some parametric model such as the weights in our linear regression example in section 5.1.4., but it can also be a whole function.
* A parameter **&#952;** can have a 'point estimate version' denoted with a hat on top aka theta-hat. Just as a unit vector v with a hat on top is called a 'v-hat'.
* Let { x&#8407;<sub>1</sub>,  x&#8407;<sub>2</sub>,  x&#8407;<sub>3</sub>, ...  x&#8407;<sub>m</sub>} be a set of *m* i.i.d. data points. A *point estimator* is any function `g()` that can take in that list of vectors as input and returns a value **&#952;-hat** 
* While almost any function qualifies as a point estimator based on the above definition, we are looking for *good* estimators. In that case, we want the output **&#952;-hat** to be close to the true underlying **&#952;**.So only estimator functions that output a reasonable **&#952;-hat** will be considered for our desired point estimator function.

#### Function Estimators
* Distinguised from point estimators above because we are not trying to just estimate a single point, but an entire function.

#### 5.4.2 Bias
#### 5.4.3 Variance aka Standard Error
* **Variance** = Var(&#952;-hat) 
* Difference betwen **Bias** from previous section 5.4.2 and **Variance** in this section.
* &#8730;<span style="text-decoration: overline; text-decoration-thickness: 1.5px"> Variance </span>= *Standard Error* = *SE(&#952;-hat)* 
    * i.e., *SE<sup>2</sup>* = *SE* * *SE* = the square of *Standard Error* = Variance
* Explaining *SE<sup>2</sup>* = Variance via Bernoulli Distribution p. 125
#### 5.4.4 Minimizing Mean Square Error by trading off between Bias and Variance
* Bias and variance measure two different sources of error in an estimator.
	* Bias measures the expected deviation from teh true value of the function or parameter.
	* OTOH, variance provides a measure of the deviation from the expected estimator value *caused specifically* by any particular sampling of the dataset.
* How do we choose between the tradeoff between bias and variance? See the total generalization error chart on p.127. GenError = BiasError + VarianceError. We can see that Bias steadily decreases as we increase model capacity. By the same token, Variance monotonically increases with model capacity.
* Therefore, there is a minimum somwhere that is the optimal capacity where the sum of BiasError + VarianceError is the lowest possible at least wrt model capacity 
* The most common way to negotiatn the tradeoff between Bias and Variance is to use cross-validation.
	* Empirically, *cross-validation is highly successful in many real-world tasks*. 
	* Alternatively, we can also compare the MSE (mean squared error) of the estimates. See equations 5.53 and 5.54 on p.126. b/c MSE = Bias<sup>2</sup> + Var, that means that when Var=0, MSE = Bias<sup>2</sup>. Conversely, when Bias<sup>2</sup>=0, then MSE = Var.
* **Desirable estimators are those with a small MSE**. These are estimators that manage have small overall MSE; i.e., these are estimators that manage to keep both their bias and variance in check.

#### 5.4.5 Consistency
* So far, we have only discussed the properties of estimators based on varying model capacity but maintaining the size of the training data set. i.e., fixed training set size.
* What happens to our estimator when we increase the amt of training data?
* Recall that the number of data examples = m. As m approaches infinity, we want to design a system st the estimator theta-hat converges on the true value of the underlying datagenerating process theta.
* The more the above bullet is true, the higher the **consistency**.
* i.e., If we are very certain that theta-hat converges on theta as m = number of examples grow, then we say there is *strong consistency*.
* i.e., If we are pretty *uncertain* that theta-hat converges on theta as m = number of examples grow, then we say there is *weak consistency*.
* Consistency ensures that the bias induced by the estimator diminishes as the number of data exaples grows.
	* However, *the converse is not true**. asymptotic unbiasedness does *not* imply consistency.

### 5.5 Maximum Likelihood Estimation
* How do we choose estimators? We can randomly pick functions and evaluate their MSE, bias, variance, consistency. But is there are more rational way of picking our initial estimator from the beginning??
* The most common principle is the **Maximum Likelihood Principle**.
* Review how squaring the maximum likelihood estimator for theta makes a simpler addition sum as opposed to a multiplicative product which is easier to manipulate. (Capital Sigma for sums is better than Capital Pi for products.) p. 128
* So what is MaxLikelihood really? One interpretation is to view it as minimizing the dissimilarity between the empericial distribution p-hat<sub>data</sub> defined by the training set and model distribution **versus** the degree of dissimilarity between training set and model distribution  as measured by KL divergence.
	* For more on **KL divergence** aka **Kullbeck-Leibler divergence**, see p.72-73 in *Section 3.13 Information Theory*. (Chapter 3 of GBC a review of Probability and Info Theory.)
* p.129 Minimizing KL divergence corresponds exactly to minimizing the cross-entropy between the distributions. Many authors use the term 'cross-entropy' refer to the negative log-likelihood of a Bernoulli or softmax distribution, *but this is a misnomer*. Any loss consisting of a negative log-likelihood is a cross-entropy between the empirical distributino defined by the training set and the probablity distribution defined by model.
	* e.g., mean square error (MSE) is the cross-entropy between the empirical distribution and the Gaussian model. 
* In other words, MaxLikelihood is an attempt to make the model distribution match the empirical distribution p-hat<sub>data</sub>
* While the optimal **&#952;** is the same regardless of whether we are **maximizing the likelihood** *or* **minimizing the KL divergence**, the values of the objective functions are different.
	* In software, we often phrase both *maximizing the likelihood* and *minimizing  the KL* as `minimizing the cost function` **J**&#8407; 
* Given the above terminology, Maximum Likelihood therefore becomes minimization of the **negative log-likelihood (NLL)**, aka minimization of cross-entropy. 
	* The perspective of maximum likelihood as minimum KL divergence becomes helpful in this ase because the KL divergence has a known minimum value of zero.
	* The NLL however *can* become negative when x&#8407; is real-valued. 
#### 5.5.1. Conditional Log-Likelihood and MSE p.129
* Note that for most supervised learning (aka training with labelled data), the maximum likelihood estimator can be easily generalized to estimate the conditional probabilty that we are looking at the correct output vector y&#8407;, given:
	1. an input vector x&#8407; 
	1. the underlying data-generating parameter **&#952;** 
		* (See p. 119 section 5.4.1 to see initial introduction of parameter **&#952;** and how it is estimated by the point estimator **&#952;-hat**.)
* See bottom of p.129 for decomposition of arg max, and p.130 for an example of how linear regression can be shown to be a maximum likelihood procedure.
#### 5.5.2 Properties of Maximum Likelihood
* p.131 The main appeal of the maximum likelihood estimator is that it can be shown to be the best estimator asymptotically. i.e., the limit as the number of training examples *m* increases to infinity, the more likely the estimator **&#952;-hat<sub>m</sub>** is to converge to the 'true value' of the platonic ideal of the parameter **&#952;**. (see p.127 again)
* Consistency (see again section 5.4.5 p.126-127)
* Two required conditions for consistency:
	1. The true distribution p<sub>data</data> must lie within the model family p<sub>model</sub>(**&#952;**)
	1. The true distribution p<sub>data</data> must correspond to *one and only one* value of **&#952;**. Otherwise, it will be impossible for the maximum likelihood to recover the correct p<sub>data</data> b/c it cannot distinguish which particular value of **&#952;** was used by the data-generating process.
* Different estimators can all have the property of consistency *but* still differ in their **statistical efficiency**.
	* The degree of *statiistical efficiency* is often studied in the **parametric case** focused on the value of a parmeter only, not on the value of an entire function.
* Thus, for reasons of *consistency* and *efficiency*, maximum likelihood is often considered the preferred estimator to use for machine learning.

### 5.6 Bayesian Statistics
* p.132 to 136
* p. 5.6.1 **Maximum A Posteriori** Estimation *aka* **MAP Estimation** p. 135


### 5.7 Supervised Learning Algorithm
* p.137 general category of **probability of output y** *p(y)* given the input of input vector x&#8407; as well as the best parameter vector **&#952;** for a parametric family of distributions.
	* Specific example of how linear regression fits into the above family.
	* For *k* number of classes, the sum of all the classes' probability needs to add up to 1. For example, assume that matter can only take on 3 phases: solid, liquid, or gas. Then p(solid) + p(liquid) + p(gas) = 1 for a given substance when we are trying to figure out what phase/class a particular compound belongs in given standard temperature and pressure. In this case, k = 3.
* For a normal Gaussian distribution over real-valued numbers, we parameterize it with the mean value.
* For a distribution over a binary variable, we have the "squash" the outcome to be between 0 and 1.
	* One way to do this is to use the logistic sigmoid function. This is called *logistic regression*
	* There is no closed-form solution to finding the optimal weights of a logistic regression. (we can't simply solve the system of equations the way we could in linear regression.) Instead, for logistic regression, we try to maximize the log-likelihood. 
	* Another way of saying maximize the log-likelihood is to minimize the NLL (negative log-likelihood) using gradient descent.

#### 5.7.2 Support Vector Machines
* p.137 One of the most influential approaches to supervised learning is the Support Vector Machine (SVM). See papers from 1992 and 1995 by Boser, Cortes, Vapnik.
* Similar to logistic regression; BUT different in that it does not output probabilities. Instead, it outputs a class identity.
* How the kernel trick works. Basically rewrite ML algorithm into dot product / inner products. p.138.
* Two benefits of the kernel trick:
	1. Lets ML algortihms learn models that are nonlinear as a function of x&#8407; using convex optimization techniques that are guaranteed to converge.
	1. The kernel function *k* often admits an implementation that is significantly more computationally efficient than naive base case. 
* The most commonly used kernel is the **Gaussian kernel**. p.139
#### Kernel machines p. 139
* SVMs introduced **the kernel trick** which can be applied to other linear models (not just support vector machines). 
	*. p.139 The category of algorithms that employ the kernel trick is known as **kernel machines** that use **kernel methods**. See Williams and Rasmussen (1996) and Schölkopf et al (1999) for more.
* A major drawback to kernel machines is that the cost of evaluating the decision function is linear with the *m* the number of training examples. SVM mitigate this by only learning an alpha vector that contains mostly zeros. Thus classifying a new example only requires evaluating the kernel function for training examples that have a non-zero alpha variable instance. These training examples are called **support vectors**. 
* Kernel machines also suffer from a high computational cost of training when the dataset is large. Kernel machines with generic kernels struggle to generalize well. For more, see sections 5.9 and 5.11.
* Kernel machines are really important to the history of deep learning. The modern incarnation of DL was designed to overcome the above limitations of kernel machines.
* In particular, the modern deep learning renaissance began when Hinton et al (2006) demonstrated that an ANN could outperform the RBF kernel SVM on the MNIST handwriting benchmark. (RBF = radial basis function at the beginning of p.139)

#### 5.7.3 Other Simple Supervised Learning Algorithms
* p.139-142 KNN = K-Nearest Neighbors and Decision-Trees.
* p.142 KNN and decision trees have many limitations *but* they are useful learning algorithms when computational resources are constrained.
* See Murphy *Machine Learning: A Probabilistic Perspective* (2012); Bishop *Pattern Recognition and Machine Learning* (2006); and *Introduction to Statistical Learning* by James, Witten, Hastie, and R. Tibshirani (2021) as good ML textbooks to learn more about traditional supervised learning algorithms.


### 5.8 Unsupervised Learning Algorithm
* add as needed

#### 5.8.1 Principal Components Analysis
* PCA
* add as needed

#### 5.8.2 k-means Clustering
* add as needed

#### DELETE
* **&#952;** = theta 
* **&#952;-hat** = theta-hat 
* "tp = **&#952;**
* "hp = **&#952;-hat**
* "p = &#8407; 
	 = vector arrow above prior character 
* function f(**x**&#8407;;**&#952;**) regularizer** to the cost function. In the case of weight decay above, the regularizer is omega(w&#8407;) = w&#8407;<sup>T</sup>w&#8407;. Essentially everything except for the &#955; lambda scalar multiplier.






### 5.9 Stochastic Gradient Descent
* Intro to SGD. Calls back to basic gradient descent algorithm from Section 4.3 (p.79) and deeper dive in Chapter 8.

### 5.10 Building an ML Algoirthm
* Nearly all deep learning algorithms can be described as particular instances of a fairly simple recipe where we combine:
	1. dataset
	1. cost function
	1. optimization procedure
	1. a model
* For example, the linear regression algorithm combines:
	1. Dataset of **X**&#8407; and y&#8407; 
	1. cost function = J(w&#8407;, b) = -Estimate<sub>x,y~p<sub>data</sub></sub>log p<sub>model</sub>(y | x)
	1. optimization procedure = solving for where the gradient of the cost function J=0 using the normal equations
	1. model specification = p<sub>model</sub>(y|x) = N(y;**x**&#8407;-transposed**w** + b, 1) 
* By realizing that we can replace any of these components mostly independently from the others, we can obtain a wide range of algorithms.
* The cost function typically includes at least one term that causes the learning process to perform statistical estimation.
	* The most common cost function is the NLL which causes the maximum likelihood estimation.
* The cost function often also includes some regularization terms. e.g., a weight decay term.
* If we cahgne the model from linear to non-linear, then most cost functions can no longer be optimized in closed form. This requires us to choose an iterative numerical optimization procedure such as gradient descent.
* In some cases, the cost function may be a function that we cannot actually evaluate, for computational reasons. In these cases, we can still approximately minimize it using iterative numerical optimization, as long as we have some way of approximating its gradients.
* Most machine learning algorithms make use of this recipe, though it may not be immediately obvious. If a machine learning algorithm seems especially unique or hand designed, it can usually be understood as using a special-case optimizer.
* Some models, such as decision trees andk-means, require special-case optimizers because their cost functions have ﬂat regions that make them inappropriate for minimization by gradient-based optimizers.
* Recognizing that most machine learning algorithms can be described using this recipe helps to see the diﬀerent algorithms as part of a taxonomy of methods for doing related tasks that work for similar reasons, rather than as a long list of algorithms that each have separate justiﬁcations.

### 5.11 Challenges that motivated deep learning
* The simple machine learning algorithms described in this chapter work well on a wide variety of important problems. They have not succeeded, however, in solving the **central problems in AI**, e.g., **(1) recognizing speech** or **(2) recognizing objects.**
* The development of deep learning was motivated in part by the failure of traditional algorithms to generalize well on such AI tasks.
* This section is about how the challenge of generalizing to new examples becomes exponentially more diﬃcult when working with high-dimensional data, and how the mechanisms used to achieve generalization in traditional machine learning are insuﬃcient to learn complicated functions in high-dimensional spaces. Such spaces also often impose high computational costs. Deep learning was designed to overcome these and other obstacles.

#### 5.11.1 The Curse of Dimensionaliy
* Diagram of 1d, 2d, and 3d and associated stastical challenge
* The more complex aka the higher the number of dimensions the input space has, the harder it is to have enough training examples to capture the true underlying probability distribution.
	* Consider the 1d version of the input space. When we try to generalize to a new datapoint within that input space, we can usually tell what to do simpyly by inspecting the training examples that already lie in that 1-d cell; presumably, there are enough examples within that cell or nearby to give us a good probability density.
	* Here are some examples:
		* **Classification task:** Return the most common class of training examples in that cell (e.g., 'dog' vs. 'cat').
		* **Estimating the probability density task:** Estimating the probability density at location x&#8407; just requires us to retrain a fraction. Numerator = number of training examples in that grid cell. Denominator = total number of training in the entire training data set.  
		* **Regression task:** Average the target values observed over the examples within that grid cell.
	* But what about when we have high-dimensional spaces; not just 3d, but n-dimensional? In these cases, the number of grid locations is massive--much larger than the number of examples in our training dataset.
	* Many traditional ML algorithms simply assume that th eoutput of a new point should be approximately teh same as the output at the nearest training point.

#### 5.11.2 Local Constancy and Smoothness Regularization
* To generalize well, ML algorithms need to be guided by prior beliefs about what kind of function they should learn. 
* We have seen these priors incorporated as explicit beliefs in the form of probability distributions over parameters of the model. 
* More informally, we may also discuss prior beliefs as directly inﬂuencing the function itself and inﬂuencing the parameters only indirectly, as a result of the relationship between the parameters and the function. 
* Additionally, we informally discuss prior beliefs as being expressed implicitly by choosing algorithms that are biased toward choosing some class of functions over another, even though these biases may not be expressed (or even be possible to express) in terms of aprobability distribution representing our degree of belief in various functions.
* Among the most widely used of these implicit *priors* is **the smoothness prior** *aka* **the local constancy prior**.
	* This prior assumes that the function we are learning does not change within a small region. (like manifolds?)
	* We explain below why the smoothness prior by itself is insufficient for many ML goals.
* Refer to Equation 5.103 (p.154). f<sup>\*</sup>(x&#8407;) &#8776; f<sup>\*</sup>(x&#8407; + **&#949;**)
* Eq. 5.103 shows a function that is smooth aka locally constant, for most configurations of x&#8407; and a small change **&#949;**. In other words, if we know a good answer for input x&#8407; then that answer is probably good in the neighborhood of x&#8407; (e.g., if x&#8407; is a labelled training example). 
* If we have several good answers in some neighborhood, we would combine them (by some form of averaging or interpolation) to produce an answer that agrees with as many of them as much as possible.
* An extreme example of the local constancy approach is the *k*-nearest neighbors family of learning algorithms. These predictors are literally constant over each region containing all the points x&#8407;  that have the same set of *k*-nearest neighbors in the training set. For *k* = 1, the number of distinguishable regions cannot be more than the number of training examples.
* While the *k*-nearest neighbors algorithm copies the output from nearby training examples, most kernel machines interpolate between training set outputs associated with nearby training examples.
* An important class of kernels is the family of **local kernels**, where **k (** u&#8407; , v&#8407; **)** has a large value when  u&#8407; and v&#8407; are similar or equal. But the further away u&#8407; and v&#8407; are from each other, the lower the value of k(). 
* A local kernel can be thought of as a similarity function that performs *template matching*, by measuring how closely a test example x&#8407; resembles each training example **x&#8407; <sup>(i)</sup>**. 
* Much of the modern motivation for deeplearning is derived from studying the limitations of local template matching andhow deep models are able to succeed in cases where local template matching fails. See Bengio, Delalleau, and Le Roux 2006 'The curse of highly variable functions for local kernel machines.' 

#### Decision trees
* Decision trees also suﬀer from the limitations of exclusively smoothness-based learning, because they break the input space into as many regions as there are leaves and use a separate parameter (or sometimes many parameters for extensions of decision trees) in each region. If the target function requires a tree with at least *n* leaves to be represented accurately, then at least *n* training examples are required to ﬁt the tree. A multiple of *n* is needed to achieve some level of statistical conﬁdence in the predicted output. 





