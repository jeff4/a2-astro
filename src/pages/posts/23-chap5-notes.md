---
layout: ../../layouts/MarkdownPostLayout.astro
title: 'Goodfellow (GBC) Chapter 5'
pubDate: 2023-11-11
description: 'Quick review of ML basics esp. statistics and related math'
author: 'Jeff'
image:
    url: ""
    alt: ""
tags: []
---


**Deep Learning (2016)** 
* Ian Goodfellow, Yoshua Bengio, and Aaron Courville
* GBC
## Chapter 5
### 5.1 Learning Algorithms
* Section 5.1.1 - The Task *T* can be many things: classification, classification with missing errors, regression, transcription, machine translation, structured output, anomaly detection, synthesis/sampling, imputation of missing values, denoising, density estimation *aka* probability mass function estimation.
* Section 5.1.2 - The Performance Measure *P*. Accuracy + Error Rate = 1.0
	* Reserving a **test set** p. 101
* Section 5.1.3 - The Experience *E*
	* Can be broadly categorized into supervised, unsupervised, and reinforcement learning
	* Rather fuzzy boundary between supervised and unsupervised learning at times. 
	* p. 103. Concept of **design matrix** which is just an object to hold many examples at once. Eg., if there are 150 plants each of which has 4 features, then you can hold that in a matrix of 4 columns and 150 rows of plants (the famous Iris leaf dataset)
* 5.1.4 - Example of Linear Regression and trying to reduce the MSE (Mean Squared Error)
	* Not just about y = **wx**, but also with the bias parameter: y = **wx** + b for the 'y-intercept'.
### 5.2 Capacity, Overfitting, and Underfitting
* p. 107 'Again we want to reduce the *training error*; so far what we have described is just an optimization problem like in linear regression.'
* '**Critical point** What separates machine learning from vanilla optimization is that we want the **test error* *aka* the **generalization error** to be reduced as well.
	* 'The definition of generalization error *aka* test error is **the expected value of the error on a *new* input.**
	* 'Here the expectation is taken across different possible inputs drawn from the distribution of inputs we expect the system to encounter in practice.
* p. 108 here is where we show why it's important there is a general similarity between the examples in the training dataset and the test data set. We must use the **i.i.d.** assumptions; the examples are *independent* of each other in the same way scross both training and test data sets; *and* the examples are *identically distributed* aka drawn from the exact same probability distribution
* Another way of expressing this is that the examples in both the training and test data sets are created by a *data-generating process*. And if the probability distribution of output examples is identical across both training and test sets, then for our purposes, we can statistically treat these two sets as being generated by the same data-gen process.
	* We call the underlying distribution the **data-generating distribution** denoted *p*<sub>data</sub>.
* The probabilisitic framework and the i.i.d. assumptions enable us to mathematically study the relationship between training error and test error (aka between training error and generalization error).
* If we are *not* engaging in ML, then given the assumptions above, we training error = test error. However, in ML, *when we use an ML algorithm, we do **not** fix the parameters ahead of time.* First, we sample the training set, then we use that to choose the parameters to reduce the training set error iteratively. Only then do we sample from the test data set. 
	* Under this process, the expected test error is greater than or equal to the expected value of traiing error.
* The factors determining how well a machine learning algorithm will perform are its ability to:
	1. Make the training error *small*
	1. Make the gap between training error and test error (generalization error) *small*
* Factor one: make the training error too large = underfitting
* Factor two: make the gap between training and test error too large = overfitting


* p. 112 chart on difference between Training Error and Generalization Error. 
#### 5.2.1 No Free Lunch Theorem
### 5.3 Hyperparameters and Validation Size
### 5.4 Estimators, Bias, and Variance
### 5.5 Maximum Likelihood Estimation
### 5.6 Bayesian Statistics
### 5.7 Supervised Learning Algorithm
### 5.8 Unsupervised Learning Algorithm
### 5.9 Stochastic Gradient Descent
### 5.10 Building an ML Algoirthm
### 5.11 Challenges that motivated deep learning




















